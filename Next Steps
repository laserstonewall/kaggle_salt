NEXT FIRST STEP: Look at the depth of the bad examples that I am getting wrong, see if that is a strongly explanatory variable.

Concrete next steps:
1. Make ensembled predictions for 0.2 downwards (except 0). Have to decide on the model. Balanced across the two classes? Maybe 48,5? 64,6? 64,5?
	For the first submission, use the main module for all 0 masks, and for 0.2 and greater. Use the chopped 64x64 model for (0,0.2]
2. Train eliminating extremely small coverage like 1 pixel. What even is that.
2a. Use the 1 class threshold, doing the 0.2 or 0.1 threshold, 0.2 brought the score down. So use the better threshold for 0.1, since the 0.2 isn't being helped by the generic threshold anyway.
2b. Use the 0.2 optimized model for 0.1 to 0.2.
3. Evaluate how the "best model" you just submitted performed on the validation sets, over the coverage profiles.

Thoughts on Next Steps:
X Individual image mean centering
X Preprocess -1 to 1
X Preprocess standard scaling
X Combine all three into channels.
Parameter search
	X Next : hit 48 on 5 depth.
	X Next: batch siz dependence at the discovered maximum: 32, 128, 256
X AdamW
Transfer learning
	First try the VGG-16 architecture, it is very similar to UNet already
Eliminate vertical masks, what are those?
Train different models:
	Vertical mask model
	Fractional coverage based models:
		0%
		(0%, 20%]
		(20,100]
		Can decide the barriers based on how my models are doing. Can first judge with the divided up model, then run a known split model with the optimal parameters found for the large images.
		How to get them to do these models? Train a model where it predicts the level of coverage? Or for images where the main model predicts low coverage, rerun the predictions with the small coverage model? Will need to see how the predictions for the main model turn out.
		This results in a 0.24 increase over an OK base model. So this may be able to help me quite a bit.
		Need to try tuning the small model. Could also try training 2 models:
			One with 0 masks included, and all masks <= 0.1 (or maybe 0.2?)
			One with just the (0.0, 0.1 masks included).
			How does including/excluding the 0.0 masks affect things? Could I use my base model, which performs pretty well on completely empty masks, and only use this subset model for the low end?
			Could also do some evaluation to add nuance, is 0.1 the best or 0.12? Can subdivide SLIGHTLY more although don't want to go crazy because I wont have enough samples.
			Could I train a model that just tries to predict the coverage category, use this as a discriminator over which model to use?
			Could I train a model that combines the predictions from the two models? Need to read more about how to do this type of model ensembling.
			I am starting to find that different models perform well for different coverage amounts. A slightly deeper model does better for the tier 2, while slightly shallower does better for tier 1. Coverage category 1 is twice as large, so I will start with using that optimized model.
			For tier 0, it may make sense to use one of the models that performed better on these, and only use its predictions if it predicts a 0. Because 0 are the biggest category, and a small increase in accuracy there will mean a big difference in the overall score. Of course, these models may tend to over assign 0s, so we may lose accuracy on the low coverage ones. We will need to test various combinations.
Train on further subdivided model, 32x32 (divide by 4)
Eliminate the vertical masks from the training set, retrain best model, 4 folds.
	Double check the completely empty masks, don't have to train on those either. May boost score a bit.
Generalized dice loss, which works better for cases when the covered fraction is smaller.
Tuning of the stochastic gradient descent / different optimization algorithms
If we go back to the original, better augmentation. Random cropping? Shift? Horizontal flip seems to help.
	Same with test time augmentation, can we do better with this?
Non droput based regularization on the layers (batch normalization is already giving me some regularization)
Pre-training now that I have added the batch normalization. I feel like this might make it go better.
Conditional random fields
Dice loss
Semi supervised models for augmentation

Principled image augmentation trials.
Upsample the classes that it is having trouble with.
Do careful cross validation of the depth/filter_scaling and grid search.

Older next steps, draw from here:
	X Zero padding instead of expansion
	X See if that layer I dropped from original UNet helps out
	X Double check all to make sure you are matching UNet, beginning and end especially
	X Deeper architecture?
	How to incorporate depth? : Stratify on this? Encode a mask with this and train to an output?
	X Mean centering images?
	Initializing weights better/different
	Change the way zero padding works in the conv net, so that it shrinks a bit
	~ Adding dropout : in both encoding and decoding phases
	~ Explore batch normalization
	Average pooling instead of max, we can deal with extra computation in this network probably.
	Playing around with the transformations
	X Remove vertical flip? Remove all transformations that screw up the "time" axis?
	X Eliminating the few "extreme" examples you saw in the initial data exploration. These could be oddly biasing the models.
	Other activation functions besides ReLU
	Change of optimizer? Maybe instead of binary crossentropy we hit MSE?
	More augmentation: random cropping, blurring, etc. Should we run the test images through augmentation too?
	Perform different image operations on the images, have different neural networks learn them, ensemble the masks somehow. This could belike one looks at inversions, one looks at things that have been sharpened, etc.
		Add multiple channels, doing something different to each channel.
	Try three different scalings: 0 to 1, -1 to 1, standardized
	Isotonic regression to correct probabilities: is this pixelwise? Need to think about isotonic regression.
		I should look at a histogram of the output predictions. Are they pushed or spread evenly? Which do I want...?
	Try ZP_1 params again with different train - test split, at current augmentation.
	Reduce all the fooling around with the gradient, use straight with long early stopping.
	Need to take a look at the type of examples that I am getting wrong, is there a certain group?
	Reapply batch normalization, but BEFORE the ReLU, since the 0's of ReLU might make more sense.