{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['lines.linewidth'] = 2\n",
    "matplotlib.rcParams['xtick.labelsize'] = 16\n",
    "matplotlib.rcParams['ytick.labelsize'] = 16\n",
    "matplotlib.rcParams['axes.labelsize'] = 20\n",
    "matplotlib.rcParams['axes.titlesize'] = 20\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Here we load in all the relevant neural network packages ###\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Input, concatenate, Conv2DTranspose, BatchNormalization\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_directory_list(directorylisting):\n",
    "    if '.DS_Store' in directorylisting:\n",
    "        directorylisting.remove('.DS_Store')\n",
    "    return directorylisting\n",
    "\n",
    "def IoU(x,y):\n",
    "    '''\n",
    "    version of IoU that uses np.bincount to get the value counts\n",
    "    \n",
    "    x and y are both numpy N x M masks\n",
    "    \n",
    "    x = proposed mask\n",
    "    y = ground truth mask\n",
    "    \n",
    "    0 for a pixel indicates the mask is blocked, 1 indicates the mask is not blocked.\n",
    "    In plain English, everywhere that is 1 we can see the cell, everywhere that is 0 we cannot.\n",
    "    \n",
    "    We want to calculate the IoU statistic, which is intersection(x,y)/union(x,y) at values where x or y is 1 \n",
    "    \n",
    "    By subtracting the proposed mask from 2 x the ground truth mask (i.e. blocked is 0, not blocked is 2),\n",
    "    then adding 1, we get unique values for each type of overlap situation, plus all values are positive, which\n",
    "    is required to use np.bincount:\n",
    "    \n",
    "INDX  0  1  2  3  4  5  6  7  8  9 10 11\n",
    "\n",
    "GT    0  0  0  2  2  2  2  2  0  0  0  0\n",
    "MSK - 0  0  1  1  1  1  0  1  1  0  0  0  \n",
    "      ----------------------------------\n",
    "      0  0 -1  1  1  1  2  1 -1  0  0  0\n",
    "    + 1  1  1  1  1  1  1  1  1  1  1  1\n",
    "      ----------------------------------\n",
    "      1  1  0  2  2  2  3  2  0  1  1  1\n",
    "      \n",
    "    0: the proposed mask had a pixel, ground truth did not (include in union)   \n",
    "    1: neither mask had a pixel (don't include)\n",
    "    2: the proposed mask had a pixed, the ground truth had a pixel (include in intersection and union)\n",
    "    3: the proposed mask did not have a pixel, the ground truth did (include in union)\n",
    "    \n",
    "    np.bincount always has length of np.amax(x) + 1, so we just need to do length checking\n",
    "    '''\n",
    "    x = x\n",
    "    y = y * 2\n",
    "    \n",
    "    diff = np.bincount((y - x + 1).flatten())\n",
    "    diff_len = len(diff)\n",
    "    \n",
    "    ### Cacluate the intersection first\n",
    "    intersection = 0\n",
    "    if (diff_len >= 3):\n",
    "        intersection = diff[2]\n",
    "    \n",
    "    ### Now calculate the union\n",
    "    union = intersection\n",
    "    if diff_len == 4:\n",
    "        union += diff[3]\n",
    "    union += diff[0]\n",
    "        \n",
    "    if union==0:\n",
    "        iou = 0 ### default value, we could potentially return blank masks, although GT should never be empty\n",
    "    else:\n",
    "        iou = float(intersection) / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "def pred_to_binary_mask(pred, threshold):\n",
    "    tst = np.zeros((pred.shape[0],pred.shape[1]), dtype=np.int8)\n",
    "    tst[pred >= threshold] = 1\n",
    "    return tst\n",
    "    \n",
    "def calc_iou(pred, gt, threshold):\n",
    "    pred_mask = pred_to_binary_mask(pred, threshold)\n",
    "    if (pred_mask.sum()==0) and (gt.sum()==0):\n",
    "        IoU_value = 1\n",
    "    elif pred.sum==0 and mask.sum()!=0:\n",
    "        IoU_value = 0\n",
    "    else:\n",
    "        IoU_value = IoU(pred_mask,gt)\n",
    "    return IoU_value\n",
    "\n",
    "def calc_avg_precision(pred_mask, gt_mask, threshold, iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]):\n",
    "    ### First calculate the IoU matrix\n",
    "    iou = calc_iou(pred_mask, gt_mask, threshold)\n",
    "    \n",
    "    avg_precision = (iou_thresholds < iou).sum() / len(iou_thresholds)\n",
    "        \n",
    "    return avg_precision\n",
    "\n",
    "class TTA_ModelWrapper_nochop():\n",
    "    \"\"\"A simple TTA wrapper for keras computer vision models.\n",
    "    Args:\n",
    "        model (keras model): A fitted keras model with a predict method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Wraps the predict method of the provided model.\n",
    "        Augments the testdata with horizontal and vertical flips and\n",
    "        averages the results.\n",
    "        Args:\n",
    "            X (numpy array of dim 4): The data to get predictions for.\n",
    "        \"\"\"\n",
    "\n",
    "        pred = []\n",
    "        for x_i in tqdm_notebook(X):\n",
    "            tmp = x_i\n",
    "            p0 = self.model.predict(tmp.reshape(1,128,128,1))\n",
    "            p1 = self.model.predict(np.fliplr(tmp).reshape(1,128,128,1))\n",
    "#             p2 = self.model.predict(np.flipud(tmp).reshape(1,128,128,1))\n",
    "#             p3 = self.model.predict(np.fliplr(np.flipud(tmp)).reshape(1,128,128,1))\n",
    "            p = (p0[0] +\n",
    "                 np.fliplr(p1[0]) #+\n",
    "#                  np.flipud(p2[0]) +\n",
    "#                  np.fliplr(np.flipud(p3[0]))\n",
    "                 ) / 2#4\n",
    "            pred.append(p)\n",
    "        return np.array(pred)\n",
    "    \n",
    "class TTA_ModelWrapper_chop():\n",
    "    \"\"\"A simple TTA wrapper for keras computer vision models.\n",
    "    Args:\n",
    "        model (keras model): A fitted keras model with a predict method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Wraps the predict method of the provided model.\n",
    "        Augments the testdata with horizontal and vertical flips and\n",
    "        averages the results.\n",
    "        Args:\n",
    "            X (numpy array of dim 4): The data to get predictions for.\n",
    "        \"\"\"\n",
    "\n",
    "        pred = []\n",
    "        for x_i in tqdm_notebook(X):\n",
    "            tmp = x_i\n",
    "            p0 = self.model.predict(tmp.reshape(1,64,64,1))\n",
    "            p1 = self.model.predict(np.fliplr(tmp).reshape(1,64,64,1))\n",
    "#             p2 = self.model.predict(np.flipud(tmp).reshape(1,128,128,1))\n",
    "#             p3 = self.model.predict(np.fliplr(np.flipud(tmp)).reshape(1,128,128,1))\n",
    "            p = (p0[0] +\n",
    "                 np.fliplr(p1[0]) #+\n",
    "#                  np.flipud(p2[0]) +\n",
    "#                  np.fliplr(np.flipud(p3[0]))\n",
    "                 ) / 2#4\n",
    "            pred.append(p)\n",
    "        return np.array(pred)\n",
    "    \n",
    "# class TTA_ModelWrapper_3channel():\n",
    "#     \"\"\"A simple TTA wrapper for keras computer vision models.\n",
    "#     Args:\n",
    "#         model (keras model): A fitted keras model with a predict method.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"Wraps the predict method of the provided model.\n",
    "#         Augments the testdata with horizontal and vertical flips and\n",
    "#         averages the results.\n",
    "#         Args:\n",
    "#             X (numpy array of dim 4): The data to get predictions for.\n",
    "#         \"\"\"\n",
    "\n",
    "#         pred = []\n",
    "#         for x_i in tqdm_notebook(X):\n",
    "#             tmp = x_i\n",
    "#             p0 = self.model.predict(tmp.reshape(1,128,128,3))\n",
    "#             p1 = self.model.predict(np.fliplr(tmp).reshape(1,128,128,3))\n",
    "# #             p2 = self.model.predict(np.flipud(tmp).reshape(1,128,128,1))\n",
    "# #             p3 = self.model.predict(np.fliplr(np.flipud(tmp)).reshape(1,128,128,1))\n",
    "#             p = (p0[0] +\n",
    "#                  np.fliplr(p1[0]) #+\n",
    "# #                  np.flipud(p2[0]) +\n",
    "# #                  np.fliplr(np.flipud(p3[0]))\n",
    "#                  ) / 2#4\n",
    "#             pred.append(p)\n",
    "#         return np.array(pred)\n",
    "\n",
    "def load_image(path):\n",
    "    img = skimage.io.imread(path, as_grey=True)\n",
    "    return img\n",
    "\n",
    "def load_mask(path):\n",
    "    mask = skimage.io.imread(path, as_grey=True)\n",
    "    mask = mask / 65535\n",
    "    return mask\n",
    "\n",
    "def biggenate(x):\n",
    "    target_image_size = (64,64,1)\n",
    "    return resize(x, target_image_size, mode='constant', preserve_range=True)\n",
    "\n",
    "def smallenate(x):\n",
    "    target_image_size = (101,101)\n",
    "    return resize(x, target_image_size, mode='constant', preserve_range=True)\n",
    "\n",
    "def biggenate_zero_pad_nochop(x):\n",
    "    target_image_size = (128,128,1)\n",
    "    new_image = np.zeros((target_image_size[0], target_image_size[1], target_image_size[2]), dtype=np.float64)\n",
    "    new_image[13:114,13:114, 0] = x\n",
    "    return new_image\n",
    "\n",
    "def biggenate_zero_pad_chop(x):\n",
    "    new_img = np.zeros((64,64,1))\n",
    "    x_ch1, x_ch2 = x.shape\n",
    "    s = 7\n",
    "    new_img[s:s+x_ch1,s:s+x_ch2,0] = x\n",
    "    return new_img\n",
    "\n",
    "def smallenate_zero_pad(x):\n",
    "    new_image = x[13:114,13:114]\n",
    "    return new_image\n",
    "\n",
    "class LearningRateHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.lr.append(K.eval(self.model.optimizer.lr))\n",
    "\n",
    "def write_submission_file(submission_file, preds_test_masks, good_test_images):\n",
    "    with(open(submission_file,'w')) as f:\n",
    "        f.write('id,rle_mask')\n",
    "\n",
    "        for i in tqdm_notebook(range(len(good_test_images))):\n",
    "            image_name = good_test_images[i].split('.')[0]\n",
    "            pred_mask = preds_test_masks[i,:,:]\n",
    "            if pred_mask.sum()==0:\n",
    "                f.write('\\n')\n",
    "                f.write(image_name + ',')\n",
    "            elif images_test_orig[i].sum()==0:\n",
    "                f.write('\\n')\n",
    "                f.write(image_name + ',')\n",
    "            else:\n",
    "                pred_mask_rle = mask_to_rle(pred_mask)\n",
    "                f.write('\\n')\n",
    "                f.write(image_name + ',' + ' '.join(list(pred_mask_rle.astype('str'))))\n",
    "\n",
    "def mask_to_rle(mask):\n",
    "    \"\"\"Takes a mask that is nxm, represented as 0 and integers, or as Boolean,\n",
    "    and converts it to run length encoding.\"\"\"\n",
    "    mask = np.transpose(mask)\n",
    "    mask = mask.astype(np.bool).astype('int')\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    dummy = np.insert(mask, 0, 0)\n",
    "    mask = np.insert(mask, len(mask), 0)\n",
    "\n",
    "    diff = dummy - mask\n",
    "    \n",
    "    diffindx = np.argwhere(diff!=0)\n",
    "    \n",
    "    starts = diffindx[::2] + 1 # it is one indexed so need this\n",
    "    \n",
    "    runs = diffindx[1::2] - diffindx[::2]\n",
    "    \n",
    "    run_length_encoded = np.hstack((starts, runs)).reshape(-1)\n",
    "    \n",
    "    return run_length_encoded\n",
    "\n",
    "def chop_up_images(image_set):\n",
    "    dummy_output = []\n",
    "    for img in image_set:\n",
    "        tst_1 = img[0:50,0:50]\n",
    "        tst_2 = img[0:50,50:]\n",
    "        tst_3 = img[50:,0:50]\n",
    "        tst_4 = img[50:,50:]\n",
    "\n",
    "        tst_1 = biggenate_zero_pad_chop(tst_1)\n",
    "        tst_2 = biggenate_zero_pad_chop(tst_2)\n",
    "        tst_3 = biggenate_zero_pad_chop(tst_3)\n",
    "        tst_4 = biggenate_zero_pad_chop(tst_4)\n",
    "\n",
    "        dummy_output.append(tst_1)\n",
    "        dummy_output.append(tst_2)\n",
    "        dummy_output.append(tst_3)\n",
    "        dummy_output.append(tst_4)\n",
    "    dummy_output = np.array(dummy_output)\n",
    "    return dummy_output\n",
    "\n",
    "def isvert(mask):\n",
    "    mask = mask.squeeze()\n",
    "    vertuni = np.unique(mask.sum(axis=1))\n",
    "    vertlen = len(np.unique(mask.sum(axis=1)))\n",
    "    if ((vertlen==1) and (vertuni[0]!=0) and (vertuni[0]!=101)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def reassemble_chopped(x1, x2, x3, x4):\n",
    "    result = np.zeros((101,101))\n",
    "    result[0:50,0:50] = x1[7:7+50,7:7+50]\n",
    "    result[0:50,50:] = x2[7:7+50,7:7+51]\n",
    "    result[50:,0:50] = x3[7:7+51,7:7+50]\n",
    "    result[50:,50:] = x4[7:7+51,7:7+51]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(filter_scaling=16, depth=5, batch_norm_momentum=0.6, input_shape=(128,128,1)):\n",
    "\n",
    "    input_layer = Input(input_shape)\n",
    "\n",
    "    conv_initialization_dict = {\"activation\":'relu', \n",
    "                                \"padding\":'same',\n",
    "                                \"kernel_initializer\" : 'he_normal'}\n",
    "\n",
    "    conv_initialization_dict_no_activation = {\"padding\":'same',\n",
    "                                \"kernel_initializer\" : 'he_normal'}\n",
    "\n",
    "    conv_dict = {}\n",
    "    for i in range(1,depth):\n",
    "        if i==1:\n",
    "            x = Conv2D((2**(i-1)) * filter_scaling, (3,3), **conv_initialization_dict_no_activation)(input_layer)\n",
    "        else:\n",
    "            x = Conv2D((2**(i-1)) * filter_scaling, (3,3), **conv_initialization_dict_no_activation)(x)\n",
    "        x = Activation('relu')(x)     \n",
    "        x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "    #     x = Dropout(0.2)(x)\n",
    "    #     x = Dropout(0.25)(x)\n",
    "        conv_dict[i] = Conv2D((2**(i-1)) * filter_scaling, (3,3), **conv_initialization_dict_no_activation)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "    #     x = Dropout(0.2)(x)\n",
    "\n",
    "        x = MaxPooling2D((2,2), padding='same')(conv_dict[i])\n",
    "\n",
    "\n",
    "    ### The bottom of the network ###\n",
    "    x = Conv2D((2**(depth-1)) * filter_scaling, (3,3), **conv_initialization_dict)(x)\n",
    "    x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "    x = Conv2D((2**(depth-1)) * filter_scaling, (3,3), **conv_initialization_dict)(x)\n",
    "    x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "\n",
    "    for i in range(depth-1,0,-1):\n",
    "\n",
    "        x = Conv2DTranspose((2**(i-1)) * filter_scaling, (3,3), strides=(2,2), **conv_initialization_dict)(x)\n",
    "        x = concatenate([conv_dict[i], x])\n",
    "        x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "        x = Conv2D((2**(i-1)) * filter_scaling, (3,3), **conv_initialization_dict)(x)\n",
    "        x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "    #     x = Dropout(0.2)(x)\n",
    "        x = Conv2D((2**(i-1)) * filter_scaling, (3,3), **conv_initialization_dict)(x)\n",
    "        if i!=1:\n",
    "            x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "    #         x = Dropout(0.2)(x)\n",
    "\n",
    "    output_layer = Conv2D(1, (1,1), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's score the main model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the test images, without chopping up for these models to make their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebc155d711d41cfa2b690941ae31173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0969fdbdc26c4bab8f85441991863728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Load in the whole list then eliminate those that are bad ###\n",
    "testdir = '../../../test/'\n",
    "all_image_files = np.array(os.listdir(filter_directory_list(testdir + 'images/')))\n",
    "good_test_images = all_image_files\n",
    "\n",
    "### Load in the images ###\n",
    "images_test_orig = np.array([load_image(testdir + 'images/' + x) for x in tqdm_notebook(good_test_images)])\n",
    "\n",
    "### Use this for expansion trained models ###\n",
    "# images_test = np.array([biggenate(x) for x in tqdm_notebook(images_test_orig)])\n",
    "\n",
    "### Use this to mean center ###\n",
    "images_test_orig = np.array([x - x.mean() for x in images_test_orig])\n",
    "\n",
    "### Use this for zero pad trained models ###\n",
    "images_test = np.array([biggenate_zero_pad_nochop(x) for x in tqdm_notebook(images_test_orig)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_threshold = pd.read_csv('../optimal_parameters_no_vertical_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filter_scaling</th>\n",
       "      <th>depth</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>fold</th>\n",
       "      <th>bn_momentum</th>\n",
       "      <th>final_epoch</th>\n",
       "      <th>random_seed</th>\n",
       "      <th>highest_val_acc</th>\n",
       "      <th>lowest_val_loss</th>\n",
       "      <th>threshold_max</th>\n",
       "      <th>mAP_max</th>\n",
       "      <th>threshold_max_augmentation</th>\n",
       "      <th>mAP_max_augmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.898782e+09</td>\n",
       "      <td>0.975430</td>\n",
       "      <td>0.070241</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.733676</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.750257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.778053e+09</td>\n",
       "      <td>0.980690</td>\n",
       "      <td>0.067851</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.791260</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.803470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.864850e+09</td>\n",
       "      <td>0.977913</td>\n",
       "      <td>0.072491</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.741388</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.757712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.302046e+09</td>\n",
       "      <td>0.980593</td>\n",
       "      <td>0.053616</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.787918</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.790874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filter_scaling  depth  batchsize  fold  bn_momentum  final_epoch  \\\n",
       "0            48.0    5.0       64.0   1.0          0.6         31.0   \n",
       "1            48.0    5.0       64.0   2.0          0.6         51.0   \n",
       "2            48.0    5.0       64.0   3.0          0.6         40.0   \n",
       "3            48.0    5.0       64.0   4.0          0.6         38.0   \n",
       "\n",
       "    random_seed  highest_val_acc  lowest_val_loss  threshold_max   mAP_max  \\\n",
       "0  3.898782e+09         0.975430         0.070241       0.653061  0.733676   \n",
       "1  1.778053e+09         0.980690         0.067851       0.591837  0.791260   \n",
       "2  1.864850e+09         0.977913         0.072491       0.530612  0.741388   \n",
       "3  3.302046e+09         0.980593         0.053616       0.551020  0.787918   \n",
       "\n",
       "   threshold_max_augmentation  mAP_max_augmentation  \n",
       "0                    0.571429              0.750257  \n",
       "1                    0.653061              0.803470  \n",
       "2                    0.571429              0.757712  \n",
       "3                    0.489796              0.790874  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_threshold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_threshold_p3 = pd.read_csv('../optimal_parameters_no_vertical_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filter_scaling</th>\n",
       "      <th>depth</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>fold</th>\n",
       "      <th>bn_momentum</th>\n",
       "      <th>final_epoch</th>\n",
       "      <th>random_seed</th>\n",
       "      <th>highest_val_acc</th>\n",
       "      <th>lowest_val_loss</th>\n",
       "      <th>threshold_max</th>\n",
       "      <th>mAP_max</th>\n",
       "      <th>threshold_max_augmentation</th>\n",
       "      <th>mAP_max_augmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.094707e+09</td>\n",
       "      <td>0.977770</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.773265</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.787018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.414748e+08</td>\n",
       "      <td>0.975223</td>\n",
       "      <td>0.066762</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.762468</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.778406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.205255e+09</td>\n",
       "      <td>0.975426</td>\n",
       "      <td>0.068618</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.743959</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.755784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.284034e+09</td>\n",
       "      <td>0.977291</td>\n",
       "      <td>0.067106</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.767738</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.775964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filter_scaling  depth  batchsize  fold  bn_momentum  final_epoch  \\\n",
       "0            48.0    5.0       64.0   1.0          0.6         36.0   \n",
       "1            48.0    5.0       64.0   2.0          0.6         33.0   \n",
       "2            48.0    5.0       64.0   3.0          0.6         35.0   \n",
       "3            48.0    5.0       64.0   4.0          0.6         37.0   \n",
       "\n",
       "    random_seed  highest_val_acc  lowest_val_loss  threshold_max   mAP_max  \\\n",
       "0  4.094707e+09         0.977770         0.065808       0.612245  0.773265   \n",
       "1  9.414748e+08         0.975223         0.066762       0.734694  0.762468   \n",
       "2  2.205255e+09         0.975426         0.068618       0.632653  0.743959   \n",
       "3  3.284034e+09         0.977291         0.067106       0.551020  0.767738   \n",
       "\n",
       "   threshold_max_augmentation  mAP_max_augmentation  \n",
       "0                    0.551020              0.787018  \n",
       "1                    0.693878              0.778406  \n",
       "2                    0.612245              0.755784  \n",
       "3                    0.551020              0.775964  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_threshold_p3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's use the top four of these trials as my \"gold standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '../unet_models/unet_optimal_parameters_m1_to_1_no_vertical_masks_fs_48_d_5_bs_64_bn_0.6_fold_2'\n",
    "model_to_load = model_file\n",
    "model = unet_model(filter_scaling=48, depth=5, batch_norm_momentum=0.6, input_shape=(128,128,1))\n",
    "model.load_weights(model_to_load)\n",
    "\n",
    "high_threshold_1 = main_threshold['threshold_max_augmentation'][1]\n",
    "print(\"Current threshold:\",high_threshold_1)\n",
    "\n",
    "### Do the model predictions\n",
    "tta = TTA_ModelWrapper_nochop(model)\n",
    "preds_test_1 = tta.predict(images_test)\n",
    "\n",
    "preds_test_orig_1 = np.array([smallenate_zero_pad(x) for x in tqdm_notebook(preds_test_1.squeeze())])\n",
    "preds_test_masks_1 = np.array([pred_to_binary_mask(x, high_threshold_1) for x in tqdm_notebook(preds_test_orig_1)])\n",
    "\n",
    "write_file = '../predicted_masks/' + model_file.split('/')[-1] + \"_preds_test_masks\"\n",
    "np.save(write_file, preds_test_masks_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current threshold: 0.4897959183673469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e916afac79f94998ab437485b357edb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f783b86819044f6dba33f5318b5bc95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d2a792cbd547b78fedf710dd5718ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_file = '../unet_models/unet_optimal_parameters_m1_to_1_no_vertical_masks_fs_48_d_5_bs_64_bn_0.6_fold_4'\n",
    "model_to_load = model_file\n",
    "model = unet_model(filter_scaling=48, depth=5, batch_norm_momentum=0.6, input_shape=(128,128,1))\n",
    "model.load_weights(model_to_load)\n",
    "\n",
    "high_threshold_2 = main_threshold['threshold_max_augmentation'][3]\n",
    "print(\"Current threshold:\",high_threshold_2)\n",
    "\n",
    "### Do the model predictions\n",
    "tta = TTA_ModelWrapper_nochop(model)\n",
    "preds_test_2 = tta.predict(images_test)\n",
    "\n",
    "preds_test_orig_2 = np.array([smallenate_zero_pad(x) for x in tqdm_notebook(preds_test_2.squeeze())])\n",
    "preds_test_masks_2 = np.array([pred_to_binary_mask(x, high_threshold_2) for x in tqdm_notebook(preds_test_orig_2)])\n",
    "\n",
    "write_file = '../predicted_masks/' + model_file.split('/')[-1] + \"_preds_test_masks\"\n",
    "np.save(write_file, preds_test_masks_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current threshold: 0.5510204081632653\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a063604c372f47b39ccb58f1224d3cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523e316228684f4f8c8adb247cc4f97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0186479e50814be19932292883adf2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_file = '../unet_models/unet_optimal_parameters_m1_to_1_no_vertical_masks_patience_3_fs_48_d_5_bs_64_bn_0.6_fold_1'\n",
    "model_to_load = model_file\n",
    "model = unet_model(filter_scaling=48, depth=5, batch_norm_momentum=0.6, input_shape=(128,128,1))\n",
    "model.load_weights(model_to_load)\n",
    "\n",
    "high_threshold_3 = main_threshold_p3['threshold_max_augmentation'][0]\n",
    "print(\"Current threshold:\",high_threshold_3)\n",
    "\n",
    "### Do the model predictions\n",
    "tta = TTA_ModelWrapper_nochop(model)\n",
    "preds_test_3 = tta.predict(images_test)\n",
    "\n",
    "preds_test_orig_3 = np.array([smallenate_zero_pad(x) for x in tqdm_notebook(preds_test_3.squeeze())])\n",
    "preds_test_masks_3 = np.array([pred_to_binary_mask(x, high_threshold_3) for x in tqdm_notebook(preds_test_orig_3)])\n",
    "\n",
    "write_file = '../predicted_masks/' + model_file.split('/')[-1] + \"_preds_test_masks\"\n",
    "np.save(write_file, preds_test_masks_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current threshold: 0.6938775510204082\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716497286fb648319b1f2f0fcec97c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004a25b476804e62b2367f1fec0f2bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe8e6a6013a4a4395192cafd51b2e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_file = '../unet_models/unet_optimal_parameters_m1_to_1_no_vertical_masks_patience_3_fs_48_d_5_bs_64_bn_0.6_fold_2'\n",
    "model_to_load = model_file\n",
    "model = unet_model(filter_scaling=48, depth=5, batch_norm_momentum=0.6, input_shape=(128,128,1))\n",
    "model.load_weights(model_to_load)\n",
    "\n",
    "high_threshold_4 = main_threshold_p3['threshold_max_augmentation'][1]\n",
    "print(\"Current threshold:\",high_threshold_4)\n",
    "\n",
    "### Do the model predictions\n",
    "tta = TTA_ModelWrapper_nochop(model)\n",
    "preds_test_4 = tta.predict(images_test)\n",
    "\n",
    "preds_test_orig_4 = np.array([smallenate_zero_pad(x) for x in tqdm_notebook(preds_test_4.squeeze())])\n",
    "preds_test_masks_4 = np.array([pred_to_binary_mask(x, high_threshold_4) for x in tqdm_notebook(preds_test_orig_4)])\n",
    "\n",
    "write_file = '../predicted_masks/' + model_file.split('/')[-1] + \"_preds_test_masks\"\n",
    "np.save(write_file, preds_test_masks_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_sum = preds_test_masks_1 + preds_test_masks_2 + preds_test_masks_3 + preds_test_masks_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_masks = (preds_test_sum >= 3).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_file = model_file + \"_preds_test_masks\"\n",
    "# np.save(write_file, preds_test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a submission file just for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d51cf709c83402cad7faccd7b673ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Write out the actual submission file ###\n",
    "submission_file = '../../submissions/unet_m1_to_1_no_vertical_combined_fs_48_d_5_bs_64_bn_0.6_all_folds_threshold_3'\n",
    "write_submission_file(submission_file, preds_test_masks, good_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now predict the masks for the low model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test_chopped = chop_up_images(images_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_threshold = pd.read_csv('./low_coverage_parameter_comparison_1.csv').reset_index()\n",
    "low_threshold.columns = (\"filter_scaling,depth,batch_size,fold,batch_norm_momentum,final_epoch,random_seed,highest_val_acc,lowest_val_loss,\" + \\\n",
    "                            \"threshold_max,mAP_max,threshold_max_class_1,mAP_max_class_1\").split(',')\n",
    "\n",
    "low_threshold = low_threshold[(low_threshold['filter_scaling']==64) & (low_threshold['depth']==5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on analysis of the effect of a score change for the different coverage categories, we will use 64,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filter_scaling</th>\n",
       "      <th>depth</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>fold</th>\n",
       "      <th>batch_norm_momentum</th>\n",
       "      <th>final_epoch</th>\n",
       "      <th>random_seed</th>\n",
       "      <th>highest_val_acc</th>\n",
       "      <th>lowest_val_loss</th>\n",
       "      <th>threshold_max</th>\n",
       "      <th>mAP_max</th>\n",
       "      <th>threshold_max_class_1</th>\n",
       "      <th>mAP_max_class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2.934359e+09</td>\n",
       "      <td>0.983985</td>\n",
       "      <td>0.055438</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.444186</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.374783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.113843e+08</td>\n",
       "      <td>0.985429</td>\n",
       "      <td>0.048578</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.455233</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.369565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.243358e+09</td>\n",
       "      <td>0.984787</td>\n",
       "      <td>0.047106</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.454651</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.382609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.182245e+09</td>\n",
       "      <td>0.983430</td>\n",
       "      <td>0.049974</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.433140</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.350435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filter_scaling  depth  batch_size  fold  batch_norm_momentum  final_epoch  \\\n",
       "24            64.0    5.0        64.0   1.0                  0.6         32.0   \n",
       "25            64.0    5.0        64.0   2.0                  0.6         44.0   \n",
       "26            64.0    5.0        64.0   3.0                  0.6         39.0   \n",
       "27            64.0    5.0        64.0   4.0                  0.6         32.0   \n",
       "\n",
       "     random_seed  highest_val_acc  lowest_val_loss  threshold_max   mAP_max  \\\n",
       "24  2.934359e+09         0.983985         0.055438       0.428571  0.444186   \n",
       "25  3.113843e+08         0.985429         0.048578       0.346939  0.455233   \n",
       "26  3.243358e+09         0.984787         0.047106       0.367347  0.454651   \n",
       "27  4.182245e+09         0.983430         0.049974       0.469388  0.433140   \n",
       "\n",
       "    threshold_max_class_1  mAP_max_class_1  \n",
       "24               0.469388         0.374783  \n",
       "25               0.408163         0.369565  \n",
       "26               0.387755         0.382609  \n",
       "27               0.469388         0.350435  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = './unet_models/unet_low_model_param_search_fs_64_d_5_bs_64_bn_0.6_fold_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = model_file\n",
    "model = unet_model(filter_scaling=64, depth=5, batch_norm_momentum=0.6, input_shape=(64,64,1))\n",
    "model.load_weights(model_to_load)\n",
    "\n",
    "high_threshold_chopped_1 = low_threshold['threshold_max'][24]\n",
    "print(\"Current threshold:\",high_threshold_chopped_1)\n",
    "\n",
    "### Do the model predictions\n",
    "preds_test_chopped_1 = model.predict(images_test_chopped)\n",
    "\n",
    "preds_test_chopped_1 = preds_test_chopped_1.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5264c6652abf46b68a4cf6af4e23e211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_test_orig_chopped_1 = []\n",
    "for i in range(len(images_test_chopped)//4):\n",
    "    img1 = preds_test_chopped_1[4*i + 0]\n",
    "    img2 = preds_test_chopped_1[4*i + 1]\n",
    "    img3 = preds_test_chopped_1[4*i + 2]\n",
    "    img4 = preds_test_chopped_1[4*i + 3]\n",
    "    preds_test_orig_chopped_1.append(reassemble_chopped(img1, img2, img3, img4))\n",
    "preds_test_orig_chopped_1 = np.array(preds_test_orig_chopped_1)\n",
    "\n",
    "preds_test_masks_chopped_1 = np.array([pred_to_binary_mask(x, high_threshold_chopped_1) for x in tqdm_notebook(preds_test_orig_chopped_1)])\n",
    "\n",
    "write_file = '../predicted_masks/' + model_file.split('/')[-1] + \"_preds_test_masks\"\n",
    "np.save(write_file, preds_test_masks_chopped_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = './unet_models/unet_low_model_param_search_fs_64_d_5_bs_64_bn_0.6_fold_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = model_file\n",
    "model = unet_model(filter_scaling=64, depth=5, batch_norm_momentum=0.6, input_shape=(64,64,1))\n",
    "model.load_weights(model_to_load)\n",
    "\n",
    "high_threshold_chopped_2 = low_threshold['threshold_max'][25]\n",
    "print(\"Current threshold:\",high_threshold_chopped_2)\n",
    "\n",
    "### Do the model predictions\n",
    "preds_test_chopped_2 = model.predict(images_test_chopped)\n",
    "\n",
    "preds_test_chopped_2 = preds_test_chopped_2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647312cbbeb54cf1af74a630a2bad9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_test_orig_chopped_2 = []\n",
    "for i in range(len(images_test_chopped)//4):\n",
    "    img1 = preds_test_chopped_2[4*i + 0]\n",
    "    img2 = preds_test_chopped_2[4*i + 1]\n",
    "    img3 = preds_test_chopped_2[4*i + 2]\n",
    "    img4 = preds_test_chopped_2[4*i + 3]\n",
    "    preds_test_orig_chopped_2.append(reassemble_chopped(img1, img2, img3, img4))\n",
    "preds_test_orig_chopped_2 = np.array(preds_test_orig_chopped_2)\n",
    "\n",
    "preds_test_masks_chopped_2 = np.array([pred_to_binary_mask(x, high_threshold_chopped_2) for x in tqdm_notebook(preds_test_orig_chopped_2)])\n",
    "\n",
    "write_file = '../predicted_masks/' + model_file.split('/')[-1] + \"_preds_test_masks\"\n",
    "np.save(write_file, preds_test_masks_chopped_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = './unet_models/unet_low_model_param_search_fs_64_d_5_bs_64_bn_0.6_fold_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = model_file\n",
    "model = unet_model(filter_scaling=64, depth=5, batch_norm_momentum=0.6, input_shape=(64,64,1))\n",
    "model.load_weights(model_to_load)\n",
    "\n",
    "high_threshold_chopped_3 = low_threshold['threshold_max'][26]\n",
    "print(\"Current threshold:\",high_threshold_chopped_3)\n",
    "\n",
    "### Do the model predictions\n",
    "preds_test_chopped_3 = model.predict(images_test_chopped)\n",
    "\n",
    "preds_test_chopped_3 = preds_test_chopped_3.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7f05e43b9a48b6834d61cc4f6934e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_test_orig_chopped_3 = []\n",
    "for i in range(len(images_test_chopped)//4):\n",
    "    img1 = preds_test_chopped_3[4*i + 0]\n",
    "    img2 = preds_test_chopped_3[4*i + 1]\n",
    "    img3 = preds_test_chopped_3[4*i + 2]\n",
    "    img4 = preds_test_chopped_3[4*i + 3]\n",
    "    preds_test_orig_chopped_3.append(reassemble_chopped(img1, img2, img3, img4))\n",
    "preds_test_orig_chopped_3 = np.array(preds_test_orig_chopped_3)\n",
    "\n",
    "preds_test_masks_chopped_3 = np.array([pred_to_binary_mask(x, high_threshold_chopped_3) for x in tqdm_notebook(preds_test_orig_chopped_3)])\n",
    "\n",
    "write_file = '../predicted_masks/' + model_file.split('/')[-1] + \"_preds_test_masks\"\n",
    "np.save(write_file, preds_test_masks_chopped_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = './unet_models/unet_low_model_param_search_fs_64_d_5_bs_64_bn_0.6_fold_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = model_file\n",
    "model = unet_model(filter_scaling=64, depth=5, batch_norm_momentum=0.6, input_shape=(64,64,1))\n",
    "model.load_weights(model_to_load)\n",
    "\n",
    "high_threshold_chopped_4 = low_threshold['threshold_max'][27]\n",
    "print(\"Current threshold:\",high_threshold_chopped_4)\n",
    "\n",
    "### Do the model predictions\n",
    "preds_test_chopped_4 = model.predict(images_test_chopped)\n",
    "preds_test_chopped_4 = preds_test_chopped_4.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30239feece049128a91090661616aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_test_orig_chopped_4 = []\n",
    "for i in range(len(images_test_chopped)//4):\n",
    "    img1 = preds_test_chopped_4[4*i + 0]\n",
    "    img2 = preds_test_chopped_4[4*i + 1]\n",
    "    img3 = preds_test_chopped_4[4*i + 2]\n",
    "    img4 = preds_test_chopped_4[4*i + 3]\n",
    "    preds_test_orig_chopped_4.append(reassemble_chopped(img1, img2, img3, img4))\n",
    "preds_test_orig_chopped_4 = np.array(preds_test_orig_chopped_4)\n",
    "\n",
    "preds_test_masks_chopped_4 = np.array([pred_to_binary_mask(x, high_threshold_chopped_4) for x in tqdm_notebook(preds_test_orig_chopped_4)])\n",
    "\n",
    "write_file = '../predicted_masks/' + model_file.split('/')[-1] + \"_preds_test_masks\"\n",
    "np.save(write_file, preds_test_masks_chopped_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reload in the masks\n",
    "preds_test_masks_1 = np.load('../predicted_masks/unet_optimal_parameters_m1_to_1_no_vertical_masks_fs_48_d_5_bs_64_bn_0.6_fold_2_preds_test_masks.npy')\n",
    "preds_test_masks_2 = np.load('../predicted_masks/unet_optimal_parameters_m1_to_1_no_vertical_masks_fs_48_d_5_bs_64_bn_0.6_fold_4_preds_test_masks.npy')\n",
    "preds_test_masks_3 = np.load('../predicted_masks/unet_optimal_parameters_m1_to_1_no_vertical_masks_patience_3_fs_48_d_5_bs_64_bn_0.6_fold_1_preds_test_masks.npy')\n",
    "preds_test_masks_4 = np.load('../predicted_masks/unet_optimal_parameters_m1_to_1_no_vertical_masks_patience_3_fs_48_d_5_bs_64_bn_0.6_fold_2_preds_test_masks.npy')\n",
    "\n",
    "preds_test_masks_chopped_1 = np.load('../predicted_masks/unet_low_model_param_search_fs_64_d_5_bs_64_bn_0.6_fold_1_preds_test_masks.npy')\n",
    "preds_test_masks_chopped_2 = np.load('../predicted_masks/unet_low_model_param_search_fs_64_d_5_bs_64_bn_0.6_fold_2_preds_test_masks.npy')\n",
    "preds_test_masks_chopped_3 = np.load('../predicted_masks/unet_low_model_param_search_fs_64_d_5_bs_64_bn_0.6_fold_3_preds_test_masks.npy')\n",
    "preds_test_masks_chopped_4 = np.load('../predicted_masks/unet_low_model_param_search_fs_64_d_5_bs_64_bn_0.6_fold_4_preds_test_masks.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_sum = preds_test_masks_1 + preds_test_masks_2 + preds_test_masks_3 + preds_test_masks_4\n",
    "preds_test_masks = (preds_test_sum >= 2).astype(np.int8)\n",
    "\n",
    "preds_test_sum_chopped = preds_test_masks_chopped_1 + preds_test_masks_chopped_2 + preds_test_masks_chopped_3 + preds_test_masks_chopped_4\n",
    "preds_test_masks_chopped = (preds_test_sum_chopped >= 2).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_masks = preds_test_masks\n",
    "# low_masks = preds_test_masks_chopped\n",
    "low_masks = np.load('../unet_models/unet_subdivide_images_1_preds_test_masks.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_masks_coverage = main_masks.sum(axis=(1,2)) / (101 * 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(main_masks_coverage); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_masks = []\n",
    "counter = 0\n",
    "for i in range(len(main_masks)):\n",
    "    if ((main_masks_coverage[i] > 0.0) and (main_masks_coverage[i] <= 0.1)):\n",
    "        combined_masks.append(low_masks[i])\n",
    "        counter += 1\n",
    "#         print(i)\n",
    "    else:\n",
    "        combined_masks.append(main_masks[i])\n",
    "combined_masks = np.array(combined_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c8b1c0433e4a7bbc52395f6a9de4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submission_file = '../../submissions/unet_combined_1_threshold_0.1_original_subdivided_mask'\n",
    "write_submission_file(submission_file, combined_masks, good_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 101, 101)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 101, 101)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 101, 101)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72000, 64, 64, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_test_chopped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 101, 101)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test_masks_chopped_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72000, 64, 64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test_chopped_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000, 101, 101), (18000, 101, 101), (18000, 101, 101), (18000, 101, 101))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test_orig_chopped_1.shape, preds_test_orig_chopped_2.shape, preds_test_orig_chopped_3.shape, preds_test_orig_chopped_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
